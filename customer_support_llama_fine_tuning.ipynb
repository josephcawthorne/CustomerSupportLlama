{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libraries and lists all files in the /kaggle/input directory\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"transformers==4.45.1\" \"accelerate==0.34.2\" \"bitsandbytes==0.44.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"datasets==3.0.1\" \"peft==0.13.0\" \"trl==0.11.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"wandb==0.18.2\" \"pyarrow==17.0.0\" \"pandas==2.2.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"huggingface_hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"peft==0.17.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y peft\n",
    "%pip install \"peft==0.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required packages in Kaggle environment (no flash-attn)\n",
    "import importlib\n",
    "\n",
    "required_packages = [\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"peft\",\n",
    "    \"trl\",\n",
    "    \"bitsandbytes\",\n",
    "    \"wandb\",\n",
    "    \"huggingface_hub\",\n",
    "    \"kaggle_secrets\",\n",
    "    \"torch\",\n",
    "    \"os\"  # standard library, always present\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "        print(f\"_/ {pkg} is installed\")\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"X {pkg} is MISSING\")\n",
    "        missing_packages.append(pkg)\n",
    "\n",
    "if missing_packages:\n",
    "    print(\"\\nSome packages are missing. Install with:\")\n",
    "    print(\"!pip install \" + \" \".join(missing_packages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required functions/methods exist in installed packages (no flash-attn)\n",
    "import importlib\n",
    "import inspect\n",
    "\n",
    "checks = {\n",
    "    \"transformers.AutoTokenizer\": \"from_pretrained\",\n",
    "    \"transformers.AutoModelForCausalLM\": \"from_pretrained\",\n",
    "    \"transformers.BitsAndBytesConfig\": \"__init__\",\n",
    "    \"transformers.HfArgumentParser\": \"__init__\",\n",
    "    \"transformers.TrainingArguments\": \"__init__\",\n",
    "    \"transformers.pipeline\": \"__call__\",\n",
    "    \"transformers.logging\": \"set_verbosity\",\n",
    "    \"transformers.MllamaForConditionalGeneration\": \"from_pretrained\",\n",
    "    \"transformers.AutoProcessor\": \"from_pretrained\",\n",
    "    \"transformers.PreTrainedTokenizer\": \"apply_chat_template\",\n",
    "    \"transformers.PreTrainedTokenizer\": \"decode\",\n",
    "    \"transformers.PreTrainedModel\": \"generate\",\n",
    "    \"transformers.PreTrainedModel\": \"save_pretrained\",\n",
    "    \"transformers.PreTrainedModel\": \"push_to_hub\",\n",
    "    \"trl.SFTTrainer\": \"__init__\",\n",
    "    \"trl\": \"setup_chat_format\",\n",
    "    \"datasets\": \"load_dataset\",\n",
    "    \"datasets.Dataset\": \"map\",\n",
    "    \"datasets.Dataset\": \"shuffle\",\n",
    "    \"datasets.Dataset\": \"select\",\n",
    "    \"peft.LoraConfig\": \"__init__\",\n",
    "    \"peft\": \"get_peft_model\",\n",
    "    \"peft.PeftModel\": \"from_pretrained\",\n",
    "    \"peft.PeftModel\": \"merge_and_unload\",\n",
    "    \"peft\": \"prepare_model_for_kbit_training\",\n",
    "    \"huggingface_hub\": \"login\",\n",
    "    \"kaggle_secrets.UserSecretsClient\": \"get_secret\",\n",
    "    \"wandb\": \"login\",\n",
    "    \"wandb\": \"init\",\n",
    "    \"wandb\": \"finish\",\n",
    "    \"torch.cuda\": \"get_device_capability\",\n",
    "    \"bitsandbytes.nn\": \"Linear4bit\"\n",
    "}\n",
    "\n",
    "for target, func in checks.items():\n",
    "    try:\n",
    "        module_name, class_or_func = target.rsplit(\".\", 1)\n",
    "        module = importlib.import_module(module_name)\n",
    "        obj = getattr(module, class_or_func)\n",
    "        if inspect.isclass(obj) or inspect.ismodule(obj):\n",
    "            if hasattr(obj, func):\n",
    "                print(f\"_/ {target}.{func} exists\")\n",
    "            else:\n",
    "                print(f\"X {target}.{func} NOT found\")\n",
    "        elif inspect.isfunction(obj):\n",
    "            print(f\"_/ Function {target} is present\")\n",
    "        else:\n",
    "            print(f\"ℹ️ {target} is present but type not checked\")\n",
    "    except Exception as e:\n",
    "        print(f\"X Could not check {target}.{func} — {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust method/function presence checker (handles module-only targets)\n",
    "import importlib\n",
    "import inspect\n",
    "\n",
    "checks = {\n",
    "    \"transformers.AutoTokenizer\": \"from_pretrained\",\n",
    "    \"transformers.AutoModelForCausalLM\": \"from_pretrained\",\n",
    "    \"transformers.BitsAndBytesConfig\": \"__init__\",\n",
    "    \"transformers.HfArgumentParser\": \"__init__\",\n",
    "    \"transformers.TrainingArguments\": \"__init__\",\n",
    "    \"transformers.pipeline\": \"__call__\",  # we'll treat this as \"callable\"\n",
    "    \"transformers.logging\": \"set_verbosity\",\n",
    "    \"transformers.MllamaForConditionalGeneration\": \"from_pretrained\",\n",
    "    \"transformers.AutoProcessor\": \"from_pretrained\",\n",
    "    \"transformers.PreTrainedTokenizer\": \"apply_chat_template\",\n",
    "    \"transformers.PreTrainedTokenizer\": \"decode\",\n",
    "    \"transformers.PreTrainedModel\": \"generate\",\n",
    "    \"transformers.PreTrainedModel\": \"save_pretrained\",\n",
    "    \"transformers.PreTrainedModel\": \"push_to_hub\",\n",
    "    \"trl\": \"setup_chat_format\",\n",
    "    \"trl.SFTTrainer\": \"__init__\",\n",
    "    \"datasets\": \"load_dataset\",\n",
    "    \"datasets.Dataset\": \"map\",\n",
    "    \"datasets.Dataset\": \"shuffle\",\n",
    "    \"datasets.Dataset\": \"select\",\n",
    "    \"peft.LoraConfig\": \"__init__\",\n",
    "    \"peft\": \"get_peft_model\",\n",
    "    \"peft.PeftModel\": \"from_pretrained\",\n",
    "    \"peft.PeftModel\": \"merge_and_unload\",\n",
    "    \"peft\": \"prepare_model_for_kbit_training\",\n",
    "    \"huggingface_hub\": \"login\",\n",
    "    \"kaggle_secrets.UserSecretsClient\": \"get_secret\",\n",
    "    \"wandb\": \"login\",\n",
    "    \"wandb\": \"init\",\n",
    "    \"wandb\": \"finish\",\n",
    "    \"torch.cuda\": \"get_device_capability\",\n",
    "    \"bitsandbytes.nn\": \"Linear4bit\",\n",
    "}\n",
    "\n",
    "for target, func in checks.items():\n",
    "    try:\n",
    "        if \".\" in target:\n",
    "            module_name, name = target.rsplit(\".\", 1)\n",
    "            mod = importlib.import_module(module_name)\n",
    "            obj = getattr(mod, name)\n",
    "        else:\n",
    "            # module-only target: import the module and use it directly\n",
    "            mod = importlib.import_module(target)\n",
    "            obj = mod\n",
    "\n",
    "        # Special case: if they asked for \"__call__\", verify it's callable\n",
    "        if func == \"__call__\":\n",
    "            if callable(obj):\n",
    "                print(f\"_/ {target} is callable\")\n",
    "            else:\n",
    "                print(f\"X {target} is not callable\")\n",
    "            continue\n",
    "\n",
    "        # For classes/modules: check attribute presence\n",
    "        if inspect.isclass(obj) or inspect.ismodule(obj):\n",
    "            if hasattr(obj, func):\n",
    "                print(f\"_/ {target}.{func} exists\")\n",
    "            else:\n",
    "                print(f\"X {target}.{func} NOT found\")\n",
    "        elif inspect.isfunction(obj):\n",
    "            # They targeted a function directly; ensure it exists\n",
    "            print(f\"_/ Function {target} is present\")\n",
    "        else:\n",
    "            # Unknown object type; try generic hasattr\n",
    "            if hasattr(obj, func):\n",
    "                print(f\"_/ {target}.{func} exists\")\n",
    "            else:\n",
    "                print(f\"X {target}.{func} NOT found\")\n",
    "    except Exception as e:\n",
    "        print(f\"X Could not check {target}.{func} — {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special handling for peft.PeftModel.merge_and_unload\n",
    "try:\n",
    "    import peft\n",
    "    from peft.peft_model import PeftModel\n",
    "    if hasattr(PeftModel, \"merge_and_unload\"):\n",
    "        print(\"_/ peft.PeftModel.merge_and_unload exists\")\n",
    "    else:\n",
    "        # check LoRA-specific model\n",
    "        try:\n",
    "            from peft.tuners.lora import LoraModel\n",
    "            if hasattr(LoraModel, \"merge_and_unload\"):\n",
    "                print(\"_/ peft.tuners.lora.LoraModel.merge_and_unload exists\")\n",
    "            else:\n",
    "                print(\"X merge_and_unload not found on PeftModel or LoraModel\")\n",
    "        except ImportError:\n",
    "            print(\"X Could not import LoraModel to check for merge_and_unload\")\n",
    "except Exception as e:\n",
    "    print(f\"X Could not check merge_and_unload — {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "# Get token from Kaggle secrets (label matches the tutorial)\n",
    "hf_token = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Login and verify identity\n",
    "login(token=hf_token)\n",
    "info = whoami()\n",
    "print(\"_/ Hugging Face connected as:\", info.get(\"name\") or info.get(\"username\") or \"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "\n",
    "# Get W&B API key from Kaggle secrets (label matches the tutorial)\n",
    "wb_token = UserSecretsClient().get_secret(\"wandb\")\n",
    "\n",
    "# Login and do a quick test run\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(project=\"connectivity-check\", job_type=\"test\", anonymous=\"allow\")\n",
    "print(\"_/ W&B run started:\", run.name, \"in project:\", run.project)\n",
    "run.finish()\n",
    "print(\"_/ W&B run finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_token = user_secrets.get_secret(\"wandb\")\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "new_model = \"llama-3.2-3b-it-Ecommerce-ChatBot\"\n",
    "dataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "def format_chat_template(row):\n",
    "    \n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction },\n",
    "               {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n",
    "    \n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= 4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting sft parameters\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "if isinstance(dataset, DatasetDict):\n",
    "    train_ds = dataset[\"train\"]\n",
    "    eval_ds  = dataset.get(\"validation\") or dataset.get(\"test\")\n",
    "elif isinstance(dataset, Dataset):         # single table\n",
    "    split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds, eval_ds = split[\"train\"], split[\"test\"]\n",
    "else:\n",
    "    raise TypeError(type(dataset))\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model_url = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "new_model_url = \"/kaggle/working/llama-3.2-3b-it-Ecommerce-ChatBot/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from trl import setup_chat_format\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_url)\n",
    "\n",
    "base_model_reload= AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_url,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge adapter with base model\n",
    "base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "model = PeftModel.from_pretrained(base_model_reload, new_model_url)\n",
    "\n",
    "#model = model.merge_and_unload()\n",
    "model = LoraModel.merge_and_unload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a top-rated customer service agent named John. \n",
    "    Be polite to customers and answer all their questions.\n",
    "    \"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"I have to see what payment payment modalities are accepted\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"llama-3.2-3b-it-Ecommerce-ChatBot\"\n",
    "\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
