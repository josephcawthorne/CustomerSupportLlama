{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I am pushing this version so it is available remotely.\n",
    "\n",
    "# Including this to make the process consistent.\n",
    "import numpy as np # linear algebra\n",
    "# This step prepares the model for the next stage.\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This line ensures the setup is in place for what follows.\n",
    "%pip uninstall -y torch torchvision torchaudio\n",
    "# Here the model is connected with the data pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Adding this part so training does not fail later.\n",
    "%pip install torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "# This configuration is needed before I move forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I am ensuring the dataset is ready for the model.\n",
    "%pip install \"transformers==4.45.1\" \"accelerate==0.34.2\" \"bitsandbytes==0.44.0\"\n",
    "# I am preparing the tokenizer so it aligns correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This is a small but necessary part of the process.\n",
    "# I am adding this step so the rest can run without issues.\n",
    "%pip install \"datasets==3.0.1\" \"peft==0.13.0\" \"trl==0.11.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Including this here keeps the flow straightforward.\n",
    "%pip install \"wandb==0.18.2\" \"pyarrow==17.0.0\" \"pandas==2.2.3\"\n",
    "# I am ensuring the dataset is ready for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This saves the model so I can use it again later.\n",
    "%pip install \"huggingface_hub\"\n",
    "# I want this section to be clear and predictable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This is sufficient for now, I will refine later.\n",
    "# Including this to make the process consistent.\n",
    "import importlib\n",
    "\n",
    "required_packages = [\n",
    "    \"transformers\",\n",
    "# I am pushing this version so it is available remotely.\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"peft\",\n",
    "    \"trl\",\n",
    "    \"bitsandbytes\",\n",
    "    \"wandb\",\n",
    "    \"huggingface_hub\",\n",
    "    \"kaggle_secrets\",\n",
    "    \"torch\",\n",
    "    \"os\"  # standard library, always present\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "        print(f\"_/ {pkg} is installed\")\n",
    "    except ModuleNotFoundError:\n",
    "        print(f\"x {pkg} is MISSING\")\n",
    "        missing_packages.append(pkg)\n",
    "\n",
    "if missing_packages:\n",
    "    print(\"\\nSome packages are missing. Install with:\")\n",
    "    print(\"!pip install \" + \" \".join(missing_packages))\n",
    "# I am checking evaluation to confirm learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Adding this part so training does not fail later.\n",
    "# I am adding this step so the rest can run without issues.\n",
    "import importlib\n",
    "# This is a small but necessary part of the process.\n",
    "import inspect\n",
    "\n",
    "checks = {\n",
    "    \"transformers.AutoTokenizer\": \"from_pretrained\",\n",
    "    \"transformers.AutoModelForCausalLM\": \"from_pretrained\",\n",
    "    \"transformers.BitsAndBytesConfig\": \"__init__\",\n",
    "    \"transformers.HfArgumentParser\": \"__init__\",\n",
    "    \"transformers.TrainingArguments\": \"__init__\",\n",
    "    \"transformers.pipeline\": \"__call__\",\n",
    "    \"transformers.logging\": \"set_verbosity\",\n",
    "    \"transformers.MllamaForConditionalGeneration\": \"from_pretrained\",\n",
    "    \"transformers.AutoProcessor\": \"from_pretrained\",\n",
    "    \"transformers.PreTrainedTokenizer\": \"apply_chat_template\",\n",
    "    \"transformers.PreTrainedTokenizer\": \"decode\",\n",
    "    \"transformers.PreTrainedModel\": \"generate\",\n",
    "    \"transformers.PreTrainedModel\": \"save_pretrained\",\n",
    "    \"transformers.PreTrainedModel\": \"push_to_hub\",\n",
    "    \"trl.SFTTrainer\": \"__init__\",\n",
    "    \"trl\": \"setup_chat_format\",\n",
    "    \"datasets\": \"load_dataset\",\n",
    "    \"datasets.Dataset\": \"map\",\n",
    "    \"datasets.Dataset\": \"shuffle\",\n",
    "    \"datasets.Dataset\": \"select\",\n",
    "    \"peft.LoraConfig\": \"__init__\",\n",
    "    \"peft\": \"get_peft_model\",\n",
    "    \"peft.PeftModel\": \"from_pretrained\",\n",
    "    \"peft.PeftModel\": \"merge_and_unload\",\n",
    "    \"peft\": \"prepare_model_for_kbit_training\",\n",
    "    \"huggingface_hub\": \"login\",\n",
    "    \"kaggle_secrets.UserSecretsClient\": \"get_secret\",\n",
    "    \"wandb\": \"login\",\n",
    "    \"wandb\": \"init\",\n",
    "    \"wandb\": \"finish\",\n",
    "    \"torch.cuda\": \"get_device_capability\",\n",
    "    \"bitsandbytes.nn\": \"Linear4bit\"\n",
    "}\n",
    "\n",
    "for target, func in checks.items():\n",
    "    try:\n",
    "        module_name, class_or_func = target.rsplit(\".\", 1)\n",
    "        module = importlib.import_module(module_name)\n",
    "        obj = getattr(module, class_or_func)\n",
    "        if inspect.isclass(obj) or inspect.ismodule(obj):\n",
    "            if hasattr(obj, func):\n",
    "                print(f\"_/ {target}.{func} exists\")\n",
    "            else:\n",
    "                print(f\"x {target}.{func} NOT found\")\n",
    "        elif inspect.isfunction(obj):\n",
    "            print(f\"_/ Function {target} is present\")\n",
    "        else:\n",
    "            print(f\"i {target} is present but type not checked\")\n",
    "    except Exception as e:\n",
    "        print(f\"x Could not check {target}.{func} â€” {e}\")\n",
    "# This section sets the arguments for training clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I am pushing this version so it is available remotely.\n",
    "%pip uninstall -y peft\n",
    "# I am preparing the tokenizer so it aligns correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This step prepares the model for the next stage.\n",
    "%pip install \"peft==0.17.1\"\n",
    "# I am checking evaluation to confirm learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I want this section to be clear and predictable.\n",
    "# Including this to make the process consistent.\n",
    "import importlib\n",
    "# I am ensuring the dataset is ready for the model.\n",
    "import inspect\n",
    "\n",
    "checks = {\n",
    "    \"transformers.AutoTokenizer\": \"from_pretrained\",\n",
    "    \"transformers.AutoModelForCausalLM\": \"from_pretrained\",\n",
    "    \"transformers.BitsAndBytesConfig\": \"__init__\",\n",
    "    \"transformers.HfArgumentParser\": \"__init__\",\n",
    "    \"transformers.TrainingArguments\": \"__init__\",\n",
    "    \"transformers.pipeline\": \"__call__\",  # we'll treat this as \"callable\"\n",
    "    \"transformers.logging\": \"set_verbosity\",\n",
    "    \"transformers.MllamaForConditionalGeneration\": \"from_pretrained\",\n",
    "    \"transformers.AutoProcessor\": \"from_pretrained\",\n",
    "    \"transformers.PreTrainedTokenizer\": \"apply_chat_template\",\n",
    "    \"transformers.PreTrainedTokenizer\": \"decode\",\n",
    "    \"transformers.PreTrainedModel\": \"generate\",\n",
    "    \"transformers.PreTrainedModel\": \"save_pretrained\",\n",
    "    \"transformers.PreTrainedModel\": \"push_to_hub\",\n",
    "    \"trl\": \"setup_chat_format\",\n",
    "    \"trl.SFTTrainer\": \"__init__\",\n",
    "    \"datasets\": \"load_dataset\",\n",
    "    \"datasets.Dataset\": \"map\",\n",
    "    \"datasets.Dataset\": \"shuffle\",\n",
    "    \"datasets.Dataset\": \"select\",\n",
    "    \"peft.LoraConfig\": \"__init__\",\n",
    "    \"peft\": \"get_peft_model\",\n",
    "    \"peft.PeftModel\": \"from_pretrained\",\n",
    "    \"peft.PeftModel\": \"merge_and_unload\",\n",
    "    \"peft\": \"prepare_model_for_kbit_training\",\n",
    "    \"huggingface_hub\": \"login\",\n",
    "    \"kaggle_secrets.UserSecretsClient\": \"get_secret\",\n",
    "    \"wandb\": \"login\",\n",
    "    \"wandb\": \"init\",\n",
    "    \"wandb\": \"finish\",\n",
    "    \"torch.cuda\": \"get_device_capability\",\n",
    "    \"bitsandbytes.nn\": \"Linear4bit\",\n",
    "}\n",
    "\n",
    "for target, func in checks.items():\n",
    "    try:\n",
    "        if \".\" in target:\n",
    "            module_name, name = target.rsplit(\".\", 1)\n",
    "            mod = importlib.import_module(module_name)\n",
    "            obj = getattr(mod, name)\n",
    "        else:\n",
    "            mod = importlib.import_module(target)\n",
    "            obj = mod\n",
    "\n",
    "        if func == \"__call__\":\n",
    "            if callable(obj):\n",
    "                print(f\"_/ {target} is callable\")\n",
    "            else:\n",
    "                print(f\"x {target} is not callable\")\n",
    "            continue\n",
    "\n",
    "        if inspect.isclass(obj) or inspect.ismodule(obj):\n",
    "            if hasattr(obj, func):\n",
    "                print(f\"_/ {target}.{func} exists\")\n",
    "            else:\n",
    "                print(f\"x {target}.{func} NOT found\")\n",
    "        elif inspect.isfunction(obj):\n",
    "            print(f\"_/ Function {target} is present\")\n",
    "        else:\n",
    "            if hasattr(obj, func):\n",
    "                print(f\"_/ {target}.{func} exists\")\n",
    "            else:\n",
    "                print(f\"x {target}.{func} NOT found\")\n",
    "    except Exception as e:\n",
    "        print(f\"x Could not check {target}.{func} â€” {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This section sets the arguments for training clearly.\n",
    "try:\n",
    "# This is a small but necessary part of the process.\n",
    "    import peft\n",
    "# This configuration is needed before I move forward.\n",
    "    from peft.peft_model import PeftModel\n",
    "    if hasattr(PeftModel, \"merge_and_unload\"):\n",
    "        print(\"_/ peft.PeftModel.merge_and_unload exists\")\n",
    "    else:\n",
    "        try:\n",
    "            from peft.tuners.lora import LoraModel\n",
    "            if hasattr(LoraModel, \"merge_and_unload\"):\n",
    "                print(\"_/ peft.tuners.lora.LoraModel.merge_and_unload exists\")\n",
    "            else:\n",
    "                print(\"x merge_and_unload not found on PeftModel or LoraModel\")\n",
    "        except ImportError:\n",
    "            print(\"x Could not import LoraModel to check for merge_and_unload\")\n",
    "except Exception as e:\n",
    "    print(f\"x Could not check merge_and_unload â€” {e}\")\n",
    "# This saves the model so I can use it again later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This line ensures the setup is in place for what follows.\n",
    "# This is sufficient for now, I will refine later.\n",
    "from transformers import (\n",
    "# This export makes the model usable in local tools.\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "# I am preparing the tokenizer so it aligns correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I am ensuring the dataset is ready for the model.\n",
    "# This step prepares the model for the next stage.\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secret_label = \"your-secret-label\"\n",
    "secret_value = UserSecretsClient().get_secret(secret_label)\n",
    "# This line ensures the setup is in place for what follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Including this here keeps the flow straightforward.\n",
    "# This configuration is needed before I move forward.\n",
    "from huggingface_hub import login\n",
    "# I am adding this step so the rest can run without issues.\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secret_label = \"HUGGINGFACE_TOKEN\"\n",
    "\n",
    "hf_token = UserSecretsClient().get_secret(secret_label)\n",
    "\n",
    "login(token=hf_token)\n",
    "# Here the model is connected with the data pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This runs the training loop to update the parameters.\n",
    "# This export makes the model usable in local tools.\n",
    "from huggingface_hub import login\n",
    "# This section sets the arguments for training clearly.\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "login(token = hf_token)\n",
    "# I am preparing the tokenizer so it aligns correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Adding this part so training does not fail later.\n",
    "# I am pushing this version so it is available remotely.\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secret_label = \"wandb\"\n",
    "secret_value = UserSecretsClient().get_secret(secret_label)\n",
    "# This is sufficient for now, I will refine later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This is a small but necessary part of the process.\n",
    "wb_token = user_secrets.get_secret(\"wandb\")\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "# I am ensuring the dataset is ready for the model.\n",
    "    project='Fine-tune Llama 3.2 on Customer Support Dataset', \n",
    "# This export makes the model usable in local tools.\n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "# I am preparing the tokenizer so it aligns correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I want this section to be clear and predictable.\n",
    "# This section sets the arguments for training clearly.\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "# Including this here keeps the flow straightforward.\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "hf_token = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "login(token=hf_token)\n",
    "info = whoami()\n",
    "print(\"_/ Hugging Face connected as:\", info.get(\"name\") or info.get(\"username\") or \"Unknown\")\n",
    "# This is a small but necessary part of the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I am checking evaluation to confirm learning progress.\n",
    "# I am pushing this version so it is available remotely.\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "# This runs the training loop to update the parameters.\n",
    "import wandb\n",
    "\n",
    "wb_token = UserSecretsClient().get_secret(\"wandb\")\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(project=\"connectivity-check\", job_type=\"test\", anonymous=\"allow\")\n",
    "print(\"_/ W&B run started:\", run.name, \"in project:\", run.project)\n",
    "run.finish()\n",
    "print(\"_/ W&B run finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Including this here keeps the flow straightforward.\n",
    "%pip uninstall -y peft\n",
    "%pip install \"peft==0.15.0\"\n",
    "# This is a small but necessary part of the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T19:43:00.695327Z",
     "iopub.status.busy": "2025-09-25T19:43:00.694653Z",
     "iopub.status.idle": "2025-09-25T19:53:54.106050Z",
     "shell.execute_reply": "2025-09-25T19:53:54.105256Z",
     "shell.execute_reply.started": "2025-09-25T19:43:00.695295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Here the model is connected with the data pipeline.\n",
    "\n",
    "RUN_LIGHTWEIGHT_INFERENCE = True       # A) Accessing Llama 3.2 Lightweight + sample prompts\n",
    "RUN_VISION_INFERENCE = False           # B) Vision demo (needs internet for sample image or provide local image)\n",
    "# I am pushing this version so it is available remotely.\n",
    "RUN_FINETUNE_3B = True                 # C) Fine-tuning 3B Instruct with QLoRA on Bitext dataset\n",
    "# Including this to make the process consistent.\n",
    "RUN_TEST_FINETUNED = True              # D) Inference with the fine-tuned (adapter) model\n",
    "RUN_MERGE_AND_PUSH = True              # E) Merge LoRA -> full model and push to Hub (requires HF token)\n",
    "\n",
    "BASE_3B_DIR = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "BASE_VISION_11B_DIR = \"/kaggle/input/llama-3.2-vision/transformers/11b-vision-instruct/1\"\n",
    "FT_MODEL_NAME = \"llama-3.2-3b-it-Ecommerce-ChatBot\"  # local dir + hub repo name\n",
    "BITEXT_DATASET = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\"\n",
    "\n",
    "SAVED_LORA_DIR = f\"/kaggle/input/fine-tune-llama-3-2-on-customer-support/{FT_MODEL_NAME}/\"\n",
    "\n",
    "import os, sys, math, json, traceback\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TextStreamer, pipeline,\n",
    "    BitsAndBytesConfig, TrainingArguments, logging as hf_logging\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    ")\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "try:\n",
    "    from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "except Exception:\n",
    "    MllamaForConditionalGeneration = None\n",
    "    AutoProcessor = None\n",
    "\n",
    "from datasets import load_dataset\n",
    "try:\n",
    "    import wandb\n",
    "except Exception:\n",
    "    wandb = None\n",
    "try:\n",
    "    from huggingface_hub import login as hf_login\n",
    "except Exception:\n",
    "    hf_login = None\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    _kaggle_secrets = UserSecretsClient()\n",
    "except Exception:\n",
    "    _kaggle_secrets = None\n",
    "\n",
    "HF_TOKEN = None\n",
    "WANDB_TOKEN = None\n",
    "\n",
    "if _kaggle_secrets:\n",
    "    try:\n",
    "        HF_TOKEN = _kaggle_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    except Exception:\n",
    "        HF_TOKEN = None\n",
    "    try:\n",
    "        WANDB_TOKEN = _kaggle_secrets.get_secret(\"wandb\")\n",
    "    except Exception:\n",
    "        WANDB_TOKEN = None\n",
    "\n",
    "if HF_TOKEN and hf_login:\n",
    "    try:\n",
    "        hf_login(token=HF_TOKEN)\n",
    "        print(\"_/ Logged in to Hugging Face Hub.\")\n",
    "    except Exception as e:\n",
    "        print(\"! HF login failed:\", e)\n",
    "\n",
    "WANDB_ENABLED = False\n",
    "if WANDB_TOKEN and wandb is not None:\n",
    "    try:\n",
    "        wandb.login(key=WANDB_TOKEN)\n",
    "        wandb_run = wandb.init(\n",
    "            project=\"Fine-tune Llama 3.2 on Customer Support Dataset\",\n",
    "            job_type=\"training\",\n",
    "            anonymous=\"allow\"\n",
    "        )\n",
    "        WANDB_ENABLED = True\n",
    "        print(\"_/ Logged in to Weights & Biases.\")\n",
    "    except Exception as e:\n",
    "        print(\"! W&B login failed:\", e)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "TORCH_DTYPE = torch.float16\n",
    "ATTN_IMPL = \"eager\"\n",
    "\n",
    "def generate_chat(model, tok, messages, max_new_tokens=200, temperature=0.8, top_p=0.95, repetition_penalty=1.1):\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token_id = tok.eos_token_id\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None:\n",
    "        model.config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "    eot_id = tok.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    eos_ids = [i for i in {model.config.eos_token_id, eot_id} if i is not None]\n",
    "\n",
    "    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            eos_token_id=eos_ids if eos_ids else None\n",
    "        )\n",
    "\n",
    "    gen_only = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tok.decode(gen_only, skip_special_tokens=True).strip()\n",
    "\n",
    "if RUN_LIGHTWEIGHT_INFERENCE:\n",
    "    print(\"\\n=== A) Lightweight 3B Inference ===\")\n",
    "    try:\n",
    "        tokenizer_lt = AutoTokenizer.from_pretrained(BASE_3B_DIR)\n",
    "\n",
    "        model_lt = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_3B_DIR,\n",
    "            return_dict=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        if tokenizer_lt.pad_token_id is None:\n",
    "            tokenizer_lt.pad_token_id = tokenizer_lt.eos_token_id\n",
    "        if model_lt.config.pad_token_id is None:\n",
    "            model_lt.config.pad_token_id = model_lt.config.eos_token_id\n",
    "\n",
    "        pipe_lt = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_lt,\n",
    "            tokenizer=tokenizer_lt,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": \"Who is Vincent van Gogh?\"}]\n",
    "        prompt = tokenizer_lt.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        out = pipe_lt(prompt, max_new_tokens=120, do_sample=True)\n",
    "        print(\"\\n[Van Gogh answer]\\n\", out[0][\"generated_text\"])\n",
    "\n",
    "        messages_sys = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a skilled Python developer specializing in database management and optimization.\"},\n",
    "            {\"role\": \"user\", \"content\": \"I'm experiencing a sorting issue in my database. Please provide Python code to help resolve this problem.\"},\n",
    "        ]\n",
    "        prompt_sys = tokenizer_lt.apply_chat_template(messages_sys, tokenize=False, add_generation_prompt=True)\n",
    "        out2 = pipe_lt(prompt_sys, max_new_tokens=512, do_sample=True)\n",
    "        text2 = out2[0][\"generated_text\"]\n",
    "        try:\n",
    "            rendered = text2.split(\"<|start_header_id|>assistant<|end_header_id|>\", 1)[1]\n",
    "        except Exception:\n",
    "            rendered = text2\n",
    "        display(Markdown(rendered))\n",
    "    except Exception as e:\n",
    "        print(\"x Lightweight inference failed:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if RUN_VISION_INFERENCE:\n",
    "    print(\"\\n=== B) Vision 11B Inference ===\")\n",
    "    if (MllamaForConditionalGeneration is None) or (AutoProcessor is None):\n",
    "        print(\"! Vision classes not available; install correct transformers build to run this section.\")\n",
    "    else:\n",
    "        try:\n",
    "            processor_v = AutoProcessor.from_pretrained(BASE_VISION_11B_DIR)\n",
    "            model_v = MllamaForConditionalGeneration.from_pretrained(\n",
    "                BASE_VISION_11B_DIR,\n",
    "                low_cpu_mem_usage=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                import requests\n",
    "                from PIL import Image\n",
    "                url = \"https://media.datacamp.com/cms/google/ad_4nxcz-j3ir2begccslzay07rqfj5ttakp2emttn0x6nkygls5ywl0unospj2s0-mrwpdtmqjl1fagh6pvkkjekqey_kwzl6qnodf143yt66znq0epflvx6clfoqw41oeoymhpz6qrlb5ajer4aeniogbmtwtd.png\"\n",
    "                image = Image.open(requests.get(url, stream=True).raw)\n",
    "            except Exception:\n",
    "                print(\"! Could not fetch remote image. Provide a local PIL image instead.\")\n",
    "                image = None\n",
    "\n",
    "            if image is not None:\n",
    "                messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Describe the tutorial feature image.\"}]}]\n",
    "                input_text = processor_v.apply_chat_template(messages, add_generation_prompt=True)\n",
    "                inputs = processor_v(image, input_text, return_tensors=\"pt\").to(model_v.device)\n",
    "                output = model_v.generate(**inputs, max_new_tokens=120)\n",
    "                print(processor_v.decode(output[0]))\n",
    "        except Exception as e:\n",
    "            print(\"x Vision inference failed:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "if RUN_FINETUNE_3B:\n",
    "    print(\"\\n=== C) Fine-tuning 3B Instruct (QLoRA) ===\")\n",
    "    try:\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=TORCH_DTYPE,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        model_ft = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_3B_DIR,\n",
    "            quantization_config=bnb_cfg,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=ATTN_IMPL\n",
    "        )\n",
    "        tok_ft = AutoTokenizer.from_pretrained(BASE_3B_DIR, trust_remote_code=True)\n",
    "\n",
    "        ds = load_dataset(BITEXT_DATASET, split=\"train\")\n",
    "        ds = ds.shuffle(seed=65).select(range(min(1000, len(ds))))  # quick demo subset\n",
    "\n",
    "        instruction = (\n",
    "            \"You are a top-rated customer service agent named John. \"\n",
    "            \"Be polite to customers and answer all their questions.\"\n",
    "        )\n",
    "\n",
    "        def to_chat_text(row):\n",
    "            row_json = [\n",
    "                {\"role\": \"system\", \"content\": instruction},\n",
    "                {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"response\"]},\n",
    "            ]\n",
    "            row[\"text\"] = tok_ft.apply_chat_template(row_json, tokenize=False)\n",
    "            return row\n",
    "\n",
    "        ds = ds.map(to_chat_text, num_proc=4)\n",
    "        split = ds.train_test_split(test_size=0.1, seed=65)\n",
    "        ds_train, ds_test = split[\"train\"], split[\"test\"]\n",
    "\n",
    "        import bitsandbytes as bnb\n",
    "        def find_all_linear_names(model_):\n",
    "            cls = bnb.nn.Linear4bit\n",
    "            lora_module_names = set()\n",
    "            for name, module in model_.named_modules():\n",
    "                if isinstance(module, cls):\n",
    "                    names = name.split(\".\")\n",
    "                    lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "            if \"lm_head\" in lora_module_names:\n",
    "                lora_module_names.remove(\"lm_head\")\n",
    "            return list(lora_module_names)\n",
    "\n",
    "        target_modules = find_all_linear_names(model_ft)\n",
    "\n",
    "        peft_cfg = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=target_modules\n",
    "        )\n",
    "        model_ft, tok_ft = setup_chat_format(model_ft, tok_ft)\n",
    "        model_ft = get_peft_model(model_ft, peft_cfg)\n",
    "\n",
    "        report_to = [\"wandb\"] if WANDB_ENABLED else []\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=FT_MODEL_NAME,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=2,\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            num_train_epochs=1,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps= max(1, math.ceil(len(ds_train) / 5)),  # evaluate ~5 times/epoch\n",
    "            logging_steps=1,\n",
    "            warmup_steps=10,\n",
    "            logging_strategy=\"steps\",\n",
    "            learning_rate=2e-4,\n",
    "            fp16=False,\n",
    "            bf16=False,\n",
    "            group_by_length=True,\n",
    "            report_to=report_to\n",
    "        )\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=model_ft,\n",
    "            train_dataset=ds_train,\n",
    "            eval_dataset=ds_test,\n",
    "            peft_config=peft_cfg,\n",
    "            max_seq_length=512,\n",
    "            dataset_text_field=\"text\",\n",
    "            tokenizer=tok_ft,\n",
    "            args=training_args,\n",
    "            packing=False,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        if WANDB_ENABLED:\n",
    "            try:\n",
    "                wandb.finish()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        os.makedirs(FT_MODEL_NAME, exist_ok=True)\n",
    "        trainer.model.save_pretrained(FT_MODEL_NAME)\n",
    "        tok_ft.save_pretrained(FT_MODEL_NAME)\n",
    "        print(f\"_/ Saved LoRA adapter + tokenizer to ./{FT_MODEL_NAME}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"x Fine-tuning failed:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if RUN_TEST_FINETUNED and RUN_FINETUNE_3B:\n",
    "    print(\"\\n=== D) Inference with Fine-tuned (adapter) ===\")\n",
    "    try:\n",
    "        if \"model_ft\" not in globals() or \"tok_ft\" not in globals():\n",
    "            tok_ft = AutoTokenizer.from_pretrained(BASE_3B_DIR, trust_remote_code=True)\n",
    "            base_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "                BASE_3B_DIR,\n",
    "                quantization_config=BitsAndBytesConfig(\n",
    "                    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=TORCH_DTYPE, bnb_4bit_use_double_quant=True\n",
    "                ),\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=ATTN_IMPL\n",
    "            )\n",
    "            base_4bit, tok_ft = setup_chat_format(base_4bit, tok_ft)\n",
    "            model_ft = PeftModel.from_pretrained(base_4bit, FT_MODEL_NAME).eval()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.\"},\n",
    "            {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}\n",
    "        ]\n",
    "\n",
    "        reply = generate_chat(model_ft, tok_ft, messages, max_new_tokens=180)\n",
    "        print(\"\\n[Fine-tuned reply]\\n\", reply)\n",
    "    except Exception:\n",
    "        print(\"x Testing fine-tuned adapter failed:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if RUN_MERGE_AND_PUSH:\n",
    "    print(\"\\n=== E) Merge Adapter into Base & Push to Hub ===\")\n",
    "    try:\n",
    "        tok_merge = AutoTokenizer.from_pretrained(BASE_3B_DIR)\n",
    "        base_full = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_3B_DIR,\n",
    "            low_cpu_mem_usage=True,\n",
    "            return_dict=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        base_full, tok_merge = setup_chat_format(base_full, tok_merge)\n",
    "\n",
    "        lora_source = SAVED_LORA_DIR if os.path.isdir(SAVED_LORA_DIR) else FT_MODEL_NAME\n",
    "        if not os.path.isdir(lora_source):\n",
    "            raise FileNotFoundError(f\"LoRA adapter directory not found: {lora_source}\")\n",
    "\n",
    "        merged = PeftModel.from_pretrained(base_full, lora_source)\n",
    "        merged = merged.merge_and_unload()\n",
    "\n",
    "        instruction = \"You are a top-rated customer service agent named John. Be polite to customers and answer all their questions.\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": \"I have to see what payment modalities are accepted\"}\n",
    "        ]\n",
    "\n",
    "        merged_reply = generate_chat(merged, tok_merge, messages, max_new_tokens=180)\n",
    "        print(\"\\n[Merged-model reply]\\n\", merged_reply)\n",
    "\n",
    "        os.makedirs(FT_MODEL_NAME, exist_ok=True)\n",
    "        merged.save_pretrained(FT_MODEL_NAME)\n",
    "        tok_merge.save_pretrained(FT_MODEL_NAME)\n",
    "        print(f\"_/ Saved merged model + tokenizer to ./{FT_MODEL_NAME}\")\n",
    "\n",
    "        if HF_TOKEN:\n",
    "            try:\n",
    "                merged.push_to_hub(FT_MODEL_NAME, use_temp_dir=False)\n",
    "                tok_merge.push_to_hub(FT_MODEL_NAME, use_temp_dir=False)\n",
    "                print(\"_/ Pushed merged model + tokenizer to Hugging Face Hub.\")\n",
    "            except Exception as e:\n",
    "                print(\"! Push to Hub failed (check permissions/repo name):\", e)\n",
    "        else:\n",
    "            print(\"i Skipping push to Hub (no HF token).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"x Merge & Push failed:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== Done ===\\nTip: Toggle the RUN_* flags at the top to select which parts to execute.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
